{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bluedata-Consulting/GAAPB02-training/blob/main/SLEFLEARNING_Prompt_Engineering_with_OpenAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZBMUH466r3l"
      },
      "source": [
        "# Prompt Engineering Techniques"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "This guide provides an introduction to prompt engineering techniques for Azure OpenAI services, focusing on how to craft effective prompts to elicit desired responses from large language models (LLMs).\n",
        "\n",
        "### Core Concepts\n",
        "\n",
        "**What is Prompt Engineering?**\n",
        "\n",
        "Prompt engineering is the art and science of designing effective prompts to guide LLMs toward generating the desired output. It's about understanding how LLMs process information and using that knowledge to create prompts that are clear, specific, and unambiguous.\n",
        "\n",
        "**Why is Prompt Engineering Important?**\n",
        "\n",
        "* **Accuracy:** Well-crafted prompts significantly improve the accuracy and relevance of the generated responses.\n",
        "* **Control:**  Prompt engineering gives you control over the style, format, and content of the output.\n",
        "* **Efficiency:** Effective prompts can minimize the number of iterations needed to achieve the desired results, saving time and resources.\n",
        "* **Customization:** Prompt engineering allows you to tailor the LLM's behavior to specific tasks and domains.\n",
        "\n",
        "\n",
        "### Key Techniques and Examples\n",
        "\n",
        "\n",
        "**1.  Clear and Specific Instructions:**\n",
        "\n",
        "* **Bad Prompt:**  \"Tell me about dogs.\"  (Too vague, could result in a wide range of responses.)\n",
        "* **Good Prompt:** \"Write a 200-word essay comparing and contrasting the characteristics of Golden Retrievers and German Shepherds.\" (Specific, sets length and topic.)\n",
        "\n",
        "\n",
        "**2.  Few-Shot Learning:**\n",
        "\n",
        "Provide examples of input-output pairs to guide the model.\n",
        "\n",
        "* **Example:**  \n"
      ],
      "metadata": {
        "id": "ttnQfHTkZJbC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make Sure, Below Environment Variables are created already:\n",
        "\n",
        "\n",
        "```\n",
        "AZURE_OPENAI_API_KEY\n",
        "AZURE_OPENAI_ENDPOINT\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "DEQ12KDFZIoh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['AZURE_OPENAI_ENDPOINT'] = \"\"\n",
        "os.environ['AZURE_OPENAI_API_KEY'] = \"\""
      ],
      "metadata": {
        "id": "d03XRAu_XflY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "HVEaMctAXC6D"
      },
      "outputs": [],
      "source": [
        "from openai import AzureOpenAI\n",
        "client = AzureOpenAI(api_version=\"2024-12-01-preview\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0WAxkVWZ7LRi"
      },
      "outputs": [],
      "source": [
        "# creating a function to get outcome\n",
        "def generate_response(prompt,model=\"telcogpt\",temperature=0):\n",
        "  messages = [{\"role\":\"user\",\"content\":prompt}]\n",
        "  response = client.chat.completions.create(\n",
        "      model = model,\n",
        "      messages = messages,\n",
        "      temperature=temperature\n",
        "  )\n",
        "  return response.choices[0].message.content\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1xrQNg0br00"
      },
      "source": [
        "## Zero Shot Prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DpqkFzDvsTe",
        "outputId": "d09df8e0-d7da-4c6d-e217-f1f5fd3bb56d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Child:** Grandma, can you tell me a story from when you were little?\n",
            "\n",
            "**Grandparent:** Oh, I’d love to! When I was your age, we didn’t have computers or tablets. We played outside all day.\n",
            "\n",
            "**Child:** Really? What did you play?\n",
            "\n",
            "**Grandparent:** We played hide and seek, climbed trees, and made up our own games. One time, my friends and I built a little fort out of sticks and leaves.\n",
            "\n",
            "**Child:** That sounds fun! Did you ever get scared?\n",
            "\n",
            "**Grandparent:** Sometimes, especially when it got dark. But we always stuck together and helped each other. It made us brave.\n",
            "\n",
            "**Child:** I wish I could play like that. I spend a lot of time on my tablet.\n",
            "\n",
            "**Grandparent:** It’s okay to use your tablet, but don’t forget to play outside too. Nature has its own magic.\n",
            "\n",
            "**Child:** I’ll try! Can we build a fort together sometime?\n",
            "\n",
            "**Grandparent:** Absolutely! Next weekend, we’ll gather some sticks and make the best fort ever.\n",
            "\n",
            "**Child:** Yay! I can’t wait! Thanks, Grandma.\n",
            "\n",
            "**Grandparent:** You’re welcome, sweetheart. I love sharing stories with you.\n"
          ]
        }
      ],
      "source": [
        "# zero shot prompting\n",
        "prompt = f\"\"\"\n",
        "Create a conversation story between child and grandparent.\n",
        "\"\"\"\n",
        "response = generate_response(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcBE11_6vwJd"
      },
      "source": [
        "## Few Shot Prompting\n",
        "\n",
        "Here, the model is given a few examples (shots) to guide its response. By providing context or previous instances, the model can better understand and generate the desired output. For example, showing a model several examples of translated sentences before asking it to translate a new one.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SurLNqfa6wi",
        "outputId": "6b5f377b-bb15-439d-d2bd-fc93fb8e7839"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<grandma>: Resilience is like a tree that bends in the wind but doesn’t break, standing tall again when the storm has passed.\n"
          ]
        }
      ],
      "source": [
        "# few shot prompting\n",
        "prompt = f\"\"\"\n",
        "your task is to answer in a consistent sytle:\n",
        "\n",
        "<child>: Teach me about patience.\n",
        "<grandma>: Patience is like waiting for whole day to see the moon in the evening and then sleeping after having a look at it.\n",
        "\n",
        "<child>: Teach me about resilience.\n",
        "\"\"\"\n",
        "response = generate_response(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqtJ2ipJbhw6"
      },
      "source": [
        "#### Few shot prompting with role setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTawSemLbhw6",
        "outputId": "981db93b-1871-4f8b-ea6a-521eb5618b7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "positive\n"
          ]
        }
      ],
      "source": [
        "result = client.chat.completions.create(\n",
        "    model='telcogpt',  # e.g. gpt-35-instant\n",
        "    max_tokens=200,\n",
        "    temperature=0.9,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"That was an awesome experience\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"positive\"},\n",
        "        {\"role\": \"user\", \"content\": \"I won't do that again\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"negative\"},\n",
        "        {\"role\": \"user\", \"content\": \"That was not worth my time\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"negative\"},\n",
        "        {\"role\": \"user\", \"content\": \"You can't miss this\"}\n",
        "    ],\n",
        ")\n",
        "print(result.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IBIItcqbhw_"
      },
      "source": [
        "## Chain of Thought Prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JbMnOeiq2atC",
        "outputId": "62b08ac7-995d-469f-e8a6-96789308d044"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Let's break down the problem step-by-step:\n",
            "\n",
            "- Marry initially had 5 pens.\n",
            "- She gave 3 pens to her friend, so she has \\(5 - 3 = 2\\) pens left.\n",
            "- She bought 2 boxes of pens, with 3 pens in each box, so she got \\(2 \\times 3 = 6\\) pens.\n",
            "- Adding these to the pens she has left: \\(2 + 6 = 8\\) pens.\n",
            "\n",
            "**Answer:** Marry has 8 pens now.\n"
          ]
        }
      ],
      "source": [
        "# Without Chain of Thought Prompting\n",
        "prompt = f\"\"\"\n",
        "\n",
        "Teacher: Johan had 5 apples, he bought 4 more boxes with 5 apples each. How many apples does he have now?\n",
        "Student: 25 apples\n",
        "\n",
        "Teacher: Marry had 5 pens, she gave 3 to her friend. and bought 2 boxes of pen with 3 pen in each box. How many pens does she have now?\n",
        "Student:\n",
        "\n",
        "\"\"\"\n",
        "response = generate_response(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7Hz_cvLbhw_",
        "outputId": "2235d9bc-1809-4a39-c860-9fa7c207d89b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Marry had 5 pens, she gave 3 to her friend, so she has 5 - 3 = 2 pens left.\n",
            "\n",
            "Then she bought 2 boxes with 3 pens in each box, so she got 2 * 3 = 6 pens.\n",
            "\n",
            "Total pens now = 2 + 6 = 8 pens.\n",
            "\n",
            "Answer: Marry has 8 pens now.\n"
          ]
        }
      ],
      "source": [
        "# with Chain of Thought prompting\n",
        "prompt = f\"\"\"\n",
        "\n",
        "Teacher: Johan had 5 apples, he bought 4 more boxes with 5 apples each. How many apples does he have now?\n",
        "Student: Johan had 5 apples, 4 boxes with 5 apples each = 5 + 4*5 = 25 apples\n",
        "\n",
        "Teacher: Marry had 5 pens, she gave 3 to her friend. and bought 2 boxes of pen with 3 pen in each box. How many pens does she have now?\n",
        "Student:\n",
        "\n",
        "\"\"\"\n",
        "response = generate_response(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7SWadXtr5ou",
        "outputId": "a7e4bda4-38e6-4fcb-81d7-bcf568a6579f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Let's analyze the problem step by step:\n",
            "\n",
            "1. **Initial number of pens Marry had:**  \n",
            "   Marry had 5 pens.\n",
            "\n",
            "2. **Pens given away:**  \n",
            "   She gave 3 pens to her friend.  \n",
            "   So, pens left = 5 - 3 = 2 pens.\n",
            "\n",
            "3. **Pens bought:**  \n",
            "   She bought 2 boxes of pens, with 3 pens in each box.  \n",
            "   Total pens bought = 2 × 3 = 6 pens.\n",
            "\n",
            "4. **Total pens now:**  \n",
            "   Pens left after giving away + pens bought = 2 + 6 = 8 pens.\n",
            "\n",
            "**Answer:** Marry now has 8 pens.\n"
          ]
        }
      ],
      "source": [
        "# with Chain of Thought prompting\n",
        "prompt = f\"\"\"\n",
        "\n",
        "Think step by step to answer below question, do not come to conclusion without following process.\n",
        "Teacher: Marry had 5 pens, she gave 3 to her friend. and bought 2 boxes of pen with 3 pen in each box. How many pens does she have now?\n",
        "Student:\n",
        "\n",
        "\"\"\"\n",
        "response = generate_response(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQvoNRxqVo5_"
      },
      "source": [
        "## Tree of Thoughts (ToT)\n",
        "\n",
        "Tree of Thoughts (ToT) prompting is a framework that generalizes over chain-of-thought prompting and encourages exploration over thoughts that serve as intermediate steps for general problem-solving with language models.\n",
        "\n",
        "\n",
        "How does it work?\n",
        "\n",
        "- ToT prompting breaks problems down into smaller parts, similar to CoT prompting, but goes further by combining this with the ability to explore multiple solution paths in parallel, forming a tree instead of a single chain. Each thought is generated or solved independently and passed to the next step, allowing the model to self-evaluate and decide whether to continue with that path or choose another."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zF7_WBM9MT78",
        "outputId": "bfb30f22-0959-4552-c990-981113febb12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Let's begin the expert panel discussion. We have 5 experts: \n",
            "\n",
            "- Expert A: Physicist specializing in mechanics and motion\n",
            "- Expert B: Cognitive psychologist specializing in human behavior and memory\n",
            "- Expert C: Forensic investigator specializing in lost item recovery\n",
            "- Expert D: Environmental scientist specializing in poolside environments\n",
            "- Expert E: Security analyst specializing in surveillance and theft prevention\n",
            "\n",
            "---\n",
            "\n",
            "### Step 1: Each expert writes down their initial reasoning step.\n",
            "\n",
            "---\n",
            "\n",
            "**Expert A (Physicist):**  \n",
            "Step 1: The watch was placed inside the towel, which Carlos carried tightly to the lounger. When Carlos shook the towel vigorously, the watch could have been dislodged and fallen out at that point. Given the laws of motion and gravity, the watch would most likely fall near the lounger, possibly on the ground or the lounger itself.\n",
            "\n",
            "Likelihood of this assertion being correct: 70%\n",
            "\n",
            "---\n",
            "\n",
            "**Expert B (Psychologist):**  \n",
            "Step 1: Carlos’s memory of the watch’s location is likely influenced by his actions and attention. He might have assumed the watch was still in the towel after shaking it, but the vigorous shaking could have caused the watch to fall out unnoticed. The most likely place he would have last had conscious awareness of the watch is at the lounger.\n",
            "\n",
            "Likelihood: 65%\n",
            "\n",
            "---\n",
            "\n",
            "**Expert C (Forensic Investigator):**  \n",
            "Step 1: The watch was last known to be inside the towel. The towel was left at the snack bar, but Carlos shook the towel at the lounger. If the watch fell out during shaking, it would be near the lounger. If it stayed in the towel, it would be at the snack bar. Given the vigorous shaking, the watch is more likely near the lounger.\n",
            "\n",
            "Likelihood: 75%\n",
            "\n",
            "---\n",
            "\n",
            "**Expert D (Environmental Scientist):**  \n",
            "Step 1: The poolside environment is wet and slippery. If the watch fell near the lounger, it might have slid or been moved by water or people walking by. However, if the watch remained in the towel, it would be at the snack bar. Environmental factors suggest the watch could have moved from the lounger area, but not far.\n",
            "\n",
            "Likelihood: 60%\n",
            "\n",
            "---\n",
            "\n",
            "**Expert E (Security Analyst):**  \n",
            "Step 1: The watch could have been stolen or misplaced. The towel was left unattended at the snack bar, a public area. If the watch was still in the towel, it could have been taken there. However, if it fell out earlier, it would be near the lounger. Theft risk is higher at the snack bar.\n",
            "\n",
            "Likelihood: 50%\n",
            "\n",
            "---\n",
            "\n",
            "### Step 2: Experts critique their own and others' responses.\n",
            "\n",
            "---\n",
            "\n",
            "**Expert A:**  \n",
            "My physics-based reasoning assumes the watch would fall out during shaking. However, I did not consider the possibility that the watch could have remained inside the towel despite shaking. Expert C’s point about the watch possibly still being in the towel at the snack bar is valid. Expert D’s environmental factors could affect the watch’s final position if it fell. Expert E’s theft possibility is lower probability but cannot be ignored.\n",
            "\n",
            "---\n",
            "\n",
            "**Expert B:**  \n",
            "I focused on Carlos’s memory and attention, which is subjective. Experts A and C provide more objective physical reasoning. I agree with Expert A that shaking could cause the watch to fall out, but I also acknowledge Expert E’s point about theft risk at the snack bar. However, theft seems less likely than accidental loss.\n",
            "\n",
            "---\n",
            "\n",
            "**Expert C:**  \n",
            "I agree with Expert A’s physics reasoning and Expert D’s environmental considerations. I think the watch is more likely near the lounger because shaking is a strong event that could dislodge the watch. Theft is less likely but possible. I will consider Expert E’s point but keep the physical evidence primary.\n",
            "\n",
            "---\n",
            "\n",
            "**Expert D:**  \n",
            "Environmental factors could cause the watch to move from the initial drop point. However, the watch is unlikely to have moved far from the lounger if it fell there. I think the watch is less likely to be at the snack bar unless it was still in the towel. I agree with Experts A and C.\n",
            "\n",
            "---\n",
            "\n",
            "**Expert E:**  \n",
            "Theft is a possibility but less likely than accidental loss. The towel was left unattended at the snack bar, which is a risk, but the watch could have fallen earlier. I will lower my theft likelihood and focus more on physical loss.\n",
            "\n",
            "---\n",
            "\n",
            "### Step 3: Experts revise their assertions based on critiques.\n",
            "\n",
            "---\n",
            "\n",
            "**Expert A:**  \n",
            "Revised Step 2: The watch was placed in the towel and shaken vigorously at the lounger. Given the force of shaking, the watch likely fell out near the lounger. Environmental factors might have moved it slightly, but it should be close to the lounger.\n",
            "\n",
            "Likelihood: 80%\n",
            "\n",
            "---\n",
            "\n",
            "**Expert B:**  \n",
            "Revised Step 2: Carlos’s memory might be faulty, but the physical event of shaking is key. The watch likely fell near the lounger, not at the snack bar. Carlos’s assumption that the watch was still in the towel at the snack bar is probably incorrect.\n",
            "\n",
            "Likelihood: 75%\n",
            "\n",
            "---\n",
            "\n",
            "**Expert C:**  \n",
            "Revised Step 2: The watch is most likely near the lounger, possibly on the ground or under the lounger. The shaking event is the critical moment. Theft is unlikely.\n",
            "\n",
            "Likelihood: 85%\n",
            "\n",
            "---\n",
            "\n",
            "**Expert D:**  \n",
            "Revised Step 2: The watch likely fell near the lounger and may have moved slightly due to environmental factors but remains close.\n",
            "\n",
            "Likelihood: 70%\n",
            "\n",
            "---\n",
            "\n",
            "**Expert E:**  \n",
            "Revised Step 2: Theft is unlikely. The watch most likely fell near the lounger during shaking.\n",
            "\n",
            "Likelihood: 65%\n",
            "\n",
            "---\n",
            "\n",
            "### Step 4: Final critique and consensus.\n",
            "\n",
            "---\n",
            "\n",
            "**Expert A:**  \n",
            "All experts agree the shaking is the critical event. I concur the watch is near the lounger.\n",
            "\n",
            "---\n",
            "\n",
            "**Expert B:**  \n",
            "Memory bias aside, physical evidence points to the lounger area.\n",
            "\n",
            "---\n",
            "\n",
            "**Expert C:**  \n",
            "I agree with the consensus.\n",
            "\n",
            "---\n",
            "\n",
            "**Expert D:**  \n",
            "Environmental factors do not contradict this conclusion.\n",
            "\n",
            "---\n",
            "\n",
            "**Expert E:**  \n",
            "I accept the physical loss explanation.\n",
            "\n",
            "---\n",
            "\n",
            "### Final Conclusion:\n",
            "\n",
            "**The single most likely location of the watch is near the lounger at the poolside, where Carlos shook the towel vigorously and the watch likely fell out.**\n",
            "\n",
            "---\n",
            "\n",
            "**Final likelihood of this conclusion: 85%**\n",
            "\n",
            "---\n",
            "\n",
            "If you want, I can help you with a plan to search for the watch or analyze further scenarios.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "prompt = \"\"\"\n",
        "Imagine 5 different experts are answering this question.\n",
        "They will brainstorm the answer step by step reasoning carefully and taking all facts into consideration\n",
        "All experts will write down 1 step of their thinking,\n",
        "then share it with the group.\n",
        "They will each critique their response, and the all the responses of others\n",
        "They will check their answer based on science and the laws of physics\n",
        "They will keep going through steps until they reach their conclusion taking into account the thoughts of the other experts\n",
        "If at any time they realise that there is a flaw in their logic they will backtrack to where that flaw occurred\n",
        "If any expert realises they're wrong at any point then they acknowledges this and start another train of thought\n",
        "Each expert will assign a likelihood of their current assertion being correct\n",
        "Continue until the experts agree on the single most likely location\n",
        "The question is.\n",
        "1. Carlos is at the swimming pool.\n",
        "2. He walks to the locker room, carrying a towel.\n",
        "3. He puts his watch in the towel and carries the towel tightly to a lounger at the poolside.\n",
        "4. At the lounger he opens and vigorously shakes the towel, then walks to the snack bar.\n",
        "5. He leaves the towel at the snack bar, then walks to the diving board.\n",
        "6. Later Carlos realises he has has lost his watch. Where is the single most likely location of the watch?\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "ans=generate_response(prompt)\n",
        "\n",
        "print(ans)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPnsQc3hpQ7x"
      },
      "source": [
        "## Chain-of-Verification Prompting\n",
        "\n",
        "The Chain-of-Verification (CoVe) prompt engineering method aims to reduce hallucinations through a verification loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9mjZtAJfDdm",
        "outputId": "66804ced-2bb2-4a20-d2ba-9c2b3ae40cc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Initial Response:**\n",
            "\n",
            "Some athletes who were born in the United States include:\n",
            "\n",
            "1. Michael Jordan (basketball)\n",
            "2. Serena Williams (tennis)\n",
            "3. Tom Brady (American football)\n",
            "4. Simone Biles (gymnastics)\n",
            "5. Tiger Woods (golf)\n",
            "\n",
            "---\n",
            "\n",
            "**Verification Questions and Answers:**\n",
            "\n",
            "1. Was Michael Jordan born in the United States?  \n",
            "   - Yes, Michael Jordan was born in Brooklyn, New York, USA.\n",
            "\n",
            "2. Was Serena Williams born in the United States?  \n",
            "   - Yes, Serena Williams was born in Saginaw, Michigan, USA.\n",
            "\n",
            "3. Was Tom Brady born in the United States?  \n",
            "   - Yes, Tom Brady was born in San Mateo, California, USA.\n",
            "\n",
            "4. Was Simone Biles born in the United States?  \n",
            "   - Yes, Simone Biles was born in Columbus, Ohio, USA.\n",
            "\n",
            "5. Was Tiger Woods born in the United States?  \n",
            "   - Yes, Tiger Woods was born in Cypress, California, USA.\n",
            "\n",
            "---\n",
            "\n",
            "**Final Verified Response:**\n",
            "\n",
            "Some athletes who were born in the United States include:\n",
            "\n",
            "1. Michael Jordan (basketball) – born in Brooklyn, New York  \n",
            "2. Serena Williams (tennis) – born in Saginaw, Michigan  \n",
            "3. Tom Brady (American football) – born in San Mateo, California  \n",
            "4. Simone Biles (gymnastics) – born in Columbus, Ohio  \n",
            "5. Tiger Woods (golf) – born in Cypress, California\n"
          ]
        }
      ],
      "source": [
        "Question=\"Name some athletes who were born in United states\"\n",
        "\n",
        "prompt=f'''Here is the question: {Question}.\n",
        "\n",
        "First, generate a response.\n",
        "\n",
        "Then, create and answer verification questions based on this response to check for accuracy. Think it through and make sure you are extremely accurate based on the question asked.\n",
        "\n",
        "After answering each verification question, consider these answers and revise the initial response to formulate a final, verified answer. Ensure the final response reflects the accuracy and findings from the verification process.'''\n",
        "\n",
        "\n",
        "\n",
        "ans=generate_response(prompt)\n",
        "\n",
        "print(ans)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8_TNNG2Ozu6"
      },
      "source": [
        "# Model Limitations: Hallucinations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6cFKxLnw5U7",
        "outputId": "96bf241e-2d86-41bf-b1ca-b286bb7e4f0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1: Get some warm water.  \n",
            "Step 2: Mix the warm water with some detergent.  \n",
            "Step 3: Put the dirty clothes into the water and detergent mixture.  \n",
            "Step 4: Let the clothes soak for some time to loosen the stains.  \n",
            "Step 5: Take the clothes out and rub the dirt or stains with a soft brush to remove any remaining stains.  \n",
            "Step 6: Rinse the clothes with clean water to remove the detergent.  \n",
            "Step 7: Hang the clothes on a rope to dry.  \n",
            "Step 8: Enjoy your clean clothes.\n"
          ]
        }
      ],
      "source": [
        "# asking for a specific format\n",
        "text = \"\"\"\n",
        "Washing clothes without washing machines is easy!\n",
        "First you need to get some warm water. Mix the warm water with some detergent. Second you can put the dirty clothes in it and let it stay wet for some time.\n",
        "During this period the stains will get loose and mixed with water.\n",
        "thereafter you can take clothes out, rub the dirt/stains with a soft brush to remove the stains if any.\n",
        "get the clothes mixed with water to get the detergent out of it. and then put it for drying on a rope.\n",
        "Enjoy the clean clothes.\n",
        "\"\"\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "you will be provided with some text delimited by triple backticks. Rewrite those into a set of instructions in the following format:\n",
        "\n",
        "Step 1: ...\n",
        "Step 2: ....\n",
        "..\n",
        "Step N:...\n",
        "\n",
        "```{text}\n",
        "```\n",
        "\"\"\"\n",
        "response = generate_response(prompt)\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIQvR5L9w5U8",
        "outputId": "a3f71bb6-88c5-4321-9736-8a8476958bc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1: Observe the morning and notice the bright sun shining between the clouds.  \n",
            "Step 2: Appreciate the pleasant weather around you.  \n",
            "Step 3: Feel the refreshing and mindblowing wind.  \n",
            "Step 4: Take a moment to enjoy and love the day.\n"
          ]
        }
      ],
      "source": [
        "# asking for a specific format\n",
        "text = \"\"\"\n",
        "THe morning today is amazing, the sun is looking bright in between clouds, the weather is pleasent and the wind is mindblowing. on a whole i am loving the day today.\n",
        "\"\"\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "you will be provided with some text delimited by triple backticks. Rewrite those into a set of instructions in the following format:\n",
        "\n",
        "Step 1: ...\n",
        "Step 2: ....\n",
        "..\n",
        "Step N:...\n",
        "\n",
        "```{text}\n",
        "```\n",
        "\"\"\"\n",
        "response = generate_response(prompt)\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVg4XE5dw5U8",
        "outputId": "d5db6ed5-c976-447a-f02b-753477924f9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1: Get some warm water.  \n",
            "Step 2: Mix the warm water with some detergent.  \n",
            "Step 3: Put the dirty clothes in the mixture and let them stay wet for some time.  \n",
            "Step 4: Take the clothes out and rub the dirt or stains with a soft brush to remove them.  \n",
            "Step 5: Rinse the clothes with water to get the detergent out.  \n",
            "Step 6: Hang the clothes on a rope to dry.  \n",
            "Step 7: Enjoy the clean clothes.\n"
          ]
        }
      ],
      "source": [
        "# asking for a specific format\n",
        "text = \"\"\"\n",
        "Washing clothes without washing machines is easy!\n",
        "First you need to get some warm water. Mix the warm water with some detergent. Second you can put the dirty clothes in it and let it stay wet for some time.\n",
        "During this period the stains will get loose and mixed with water.\n",
        "thereafter you can take clothes out, rub the dirt/stains with a soft brush to remove the stains if any.\n",
        "get the clothes mixed with water to get the detergent out of it. and then put it for drying on a rope.\n",
        "Enjoy the clean clothes.\n",
        "\"\"\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "you will be provided with some text delimited by triple backticks.\n",
        "Rewrite those into a set of instructions in the following format only if contains a sequence of instructions:\n",
        "\n",
        "Step 1: ...\n",
        "Step 2: ....\n",
        "..\n",
        "Step N:...\n",
        "\n",
        "```{text}\n",
        "```\n",
        "\n",
        "if the text does not contain a sequence of instructions, then simply write 'No steps provided'\n",
        "\"\"\"\n",
        "response = generate_response(prompt)\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTNZlh8kw5U9",
        "outputId": "48744b8e-14bd-48f5-efde-b86c218676b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No steps provided\n"
          ]
        }
      ],
      "source": [
        "# asking for a specific format\n",
        "text = \"\"\"\n",
        "THe morning today is amazing, the sun is looking bright in between clouds, the weather is pleasent and the wind is mindblowing. on a whole i am loving the day today.\n",
        "\"\"\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "you will be provided with some text delimited by triple backticks.\n",
        "Rewrite those into a set of instructions in the following format only if contains a sequence of instructions:\n",
        "\n",
        "Step 1: ...\n",
        "Step 2: ....\n",
        "..\n",
        "Step N:...\n",
        "\n",
        "```{text}\n",
        "```\n",
        "\n",
        "if the text does not contain a sequence of instructions, then simply write 'No steps provided'\n",
        "\"\"\"\n",
        "response = generate_response(prompt)\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "Or9Q36oxO4bx",
        "outputId": "760e0044-2f72-4a8b-83ec-79d5d6d69fbe"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I couldn't find specific information about an ergonomic chair produced and sold by Morning Star IT Services. It's possible that this product is new, niche, or not widely documented online.\\n\\nIf you have any additional details or context about the chair, such as a model name or features, please share them, and I can help you better. Alternatively, you might want to check Morning Star IT Services' official website or contact them directly for the most accurate and detailed information about their ergonomic chair.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "prompt = f\"\"\"\n",
        "Tell me about Ergonamic chair procuced and sold by Morning Star IT Services.\n",
        "\"\"\"\n",
        "\n",
        "generate_response(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "n4rvOOeSPIDV",
        "outputId": "60f862e1-c39e-4143-ad02-e6eeb6f4c9fa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"The **Ergonomic Toothbrush by Colgate** is designed to provide a comfortable and effective brushing experience by focusing on user-friendly design features. While specific models may vary, here are some common characteristics of Colgate's ergonomic toothbrushes:\\n\\n1. **Comfortable Handle Design**: The handle is typically shaped to fit comfortably in the hand, often featuring non-slip grips or rubberized sections to ensure better control and reduce hand fatigue during brushing.\\n\\n2. **Optimized Bristle Arrangement**: The bristles are arranged to reach different areas of the mouth effectively, including hard-to-reach spots, helping to remove plaque and promote better oral hygiene.\\n\\n3. **Flexible Neck**: Some ergonomic toothbrushes include a flexible neck that helps absorb excess pressure while brushing, protecting gums from damage.\\n\\n4. **Variety of Bristle Types**: Options may include soft, medium, or firm bristles, catering to different user preferences and dental needs.\\n\\n5. **Durability and Quality**: As a product from Colgate, a trusted oral care brand, the ergonomic toothbrush is made with quality materials to ensure durability and effective cleaning.\\n\\nIf you are looking for a specific model or additional features like battery-powered ergonomic toothbrushes or those with special bristle technology, please let me know!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "prompt = f\"\"\"\n",
        "Tell me about Ergonamic Toothbrush by colgate.\n",
        "\"\"\"\n",
        "\n",
        "generate_response(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "hzawWC-7PT_l",
        "outputId": "26916207-9514-4f25-f96e-c28d9b038b2b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The Ultraslim Toothbrush by MorningStar is designed to provide effective oral care with a sleek and slim profile. Key features typically include:\\n\\n- **Ultra-thin handle:** Designed for a comfortable grip and easy maneuverability, allowing you to reach all areas of your mouth.\\n- **Soft bristles:** Gentle on gums while effectively cleaning teeth and removing plaque.\\n- **Compact head:** Helps access hard-to-reach areas, promoting thorough cleaning.\\n- **Durability:** Made with quality materials to ensure long-lasting use.\\n\\nMorningStar’s Ultraslim Toothbrush is often praised for combining functionality with a minimalist design, making it a popular choice for those seeking a simple yet effective toothbrush.\\n\\nIf you want specific details like pricing, availability, or user reviews, please let me know!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "prompt = f\"\"\"\n",
        "Tell me about Ultraslim Toothbrush by MorningStar.\n",
        "\"\"\"\n",
        "\n",
        "generate_response(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjWPGpKzPdQB",
        "outputId": "3bf2ed3f-b666-47f4-fa62-a4da5f8a3462"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Manila: The vibrant capital of the Philippines, known for its rich history, bustling streets, and cultural landmarks.  \n",
            "2. Delhi: India's sprawling capital, blending ancient heritage with modern urban life and political significance.  \n",
            "3. MorningStarCity: A futuristic metropolis renowned for its innovative technology, sustainable living, and dynamic skyline.\n"
          ]
        }
      ],
      "source": [
        "prompt = f\"\"\"\n",
        "Generate one line description for each of the following city:\n",
        "1. Manila\n",
        "2. Delhi\n",
        "3. MorningStarCity\n",
        "\"\"\"\n",
        "\n",
        "print(generate_response(prompt))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Io6r-CoeRP5r",
        "outputId": "3fcfaa3a-fe9c-46d0-e335-1096e58d8654"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Manila: The capital city of the Philippines, known for its rich history and vibrant urban culture.  \n",
            "2. Delhi: The bustling capital territory of India, famous for its historic monuments and diverse culture.  \n",
            "3. MorningStarCity: No description can be generated.\n"
          ]
        }
      ],
      "source": [
        "prompt = f\"\"\"\n",
        "Generate one line description for each of the following city, check if the city exists, if it does not exist, say no description can be generated:\n",
        "1. Manila\n",
        "2. Delhi\n",
        "3. MorningStarCity\n",
        "\"\"\"\n",
        "\n",
        "print(generate_response(prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-DmK07y94x8"
      },
      "source": [
        "## Conversational App"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Lap6MZ-ARdKd"
      },
      "outputs": [],
      "source": [
        "# creating a function to get outcome\n",
        "def generate_response(messages,model='telcogpt'):\n",
        "  response = client.chat.completions.create(\n",
        "      model = model,\n",
        "      messages = messages,\n",
        "      temperature=0.5\n",
        "  )\n",
        "  return response.choices[0].message.content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "zgHXbLb0bhxA",
        "outputId": "f1e2c694-110a-4195-ed59-870d4d1c818b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You: hi\n",
            "You:  hi\n",
            "Bot: Hello! How can I assist you today?\n",
            "You: exit()\n",
            "You:  exit()\n",
            "Bot: If you want to end our conversation, that's perfectly fine. Have a great day! If you need any help later, feel free to reach out. Goodbye!\n",
            "You: exit\n",
            "You:  exit\n"
          ]
        }
      ],
      "source": [
        "messages = [{\"role\":\"system\",\"content\":\"You are a helpful assistant\"},]\n",
        "while True:\n",
        "    uinput = input(\"You: \")\n",
        "    print(\"You: \",uinput)\n",
        "    if uinput.lower() in ['q','quit','exit']:\n",
        "        break\n",
        "    messages.append({\"role\":\"user\",\"content\":uinput})\n",
        "    response = generate_response(messages=messages)\n",
        "    print(f\"Bot: {response}\")\n",
        "    messages.append({\"role\":\"assistant\",\"content\":response})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3TANJVdDVl8l"
      },
      "outputs": [],
      "source": [
        "messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPJ1Sk2PVl8m"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WErsqIf94x9"
      },
      "source": [
        "## Thank You"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}